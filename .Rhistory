NASDAQpath <- paste0('/Users/cojamalo/Downloads/NASDAQ_',format(day, '%Y%m%d'), '.csv')
##Call to read_csv, formatting, and insert
if (InsertEOD(my_db, NYSEpath)) {
print(paste(NYSEpath, "was successfully imported!"))
}
if (InsertEOD(my_db, NASDAQpath)) {
print(paste(NASDAQpath, "was successfully imported!"))
}
#InsertEOD(my_db, NYSEpath)
#InsertEOD(my_db, NASDAQpath)
##Import success check
earlydate <- Sys.Date() - 5
my_db %>% tbl("master") %>% filter((symbol == "AAPL" | symbol == "XOM") & date > earlydate) %>% arrange(desc(date)) %>% tbl_df %>% glimpse
earlydate <- Sys.Date() - 600
#Collect the latest list of all stock symbols
full <- my_db %>% tbl("master") %>% filter(date > earlydate) %>% select(symbol, date, close) %>% arrange(date) %>% tbl_df
#Generate symbol names with enough data
names <- full %>% group_by(symbol) %>% summarize(count = sum(complete.cases(close))) %>% filter(count > 199)
names <- factor(names$symbol)
names <- levels(names)
#Generate data needed from lsit of names
dat1 <- full %>% filter(symbol %in% names) %>% tbl_df
#Initalize result lists
result <- data.frame(symbol = "MATRIX", date = as.Date("1970-01-01"), overbot = 0, oversold = 0)
for (name in names) {
#Initalize variables
overbot <- 0
oversold <- 0
#Pull data
sample <- dat1 %>% filter((symbol == name))
sample <- sample[complete.cases(sample),]
sample <- sample %>% mutate(EMA150 = EMA(sample$close, n = 150), RSI5 = RSI(sample$close, n = 5), Slope2 = EMA150 - lag(EMA150, default = first(EMA150)))
last_row <- sample[dim(sample)[1],]
if (last_row$RSI5 > 80 & last_row$Slope2 > 0) {
overbot <- 1
}
if(last_row$RSI5 < 20 & last_row$Slope2 < 0) {
oversold <- 1
}
new_row <- data.frame(symbol = name, date = last_row$date, overbot = overbot, oversold = oversold)
result <- rbind(result, new_row)
}
result$overbot <- as.integer(result$overbot)
result$oversold <- as.integer(result$oversold)
new_heat <- result %>% group_by(date) %>% summarize(overbot = sum(overbot), oversold = sum(oversold)) %>% arrange(desc(date))
new_heat <- new_heat[1,] %>% mutate(diff = overbot-oversold)
glimpse(new_heat)
heat <- fread("/Users/cojamalo/Documents/R/Stocks/heat_index.csv", sep = ",") %>% select(-V1) %>% tbl_df
heat$date <- ymd(heat$date)
heat <- rbind(heat, new_heat)
write.csv(heat,file='/Users/cojamalo/Documents/R/Stocks/heat_index.csv', sep = ",")
tail(heat)
ggplot(heat, aes(x=date)) + geom_area(aes(y = overbot), fill = "green", color= "green", alpha= 0.5) + geom_area(aes(y = oversold), fill = "red",  color= "red",alpha= 0.5) + xlim(c((Sys.Date()-(365*window)), Sys.Date()))
heat <- mutate(heat, EMA6 = EMA(diff, n = 6))
params <- heat %>% summarize(Q1 = quantile(diff, 0.25, na.rm = TRUE), MEAN = mean(diff, na.rm = TRUE), MEDIAN = median(diff, na.rm = TRUE),Q3 = quantile(diff, 0.75, na.rm = TRUE), IQR = IQR(diff, na.rm = TRUE), STDEV = sd(diff, na.rm = TRUE)) %>%
mutate(SKEW = ifelse(MEAN > MEDIAN, "RIGHT", "LEFT"))
cc <- scales::div_gradient_pal("red","#D9D2E9", "#00FFFF")(seq(0,1,length.out=5))
ggplot(heat, aes(x = date, y = diff)) +
geom_rect(aes(xmin = date, xmax = date, ymax = 1200), ymin = params$Q3, color = cc[1]) +
geom_rect(aes(xmin = date, xmax = date), ymax = params$Q3, ymin = params$MEAN, color = cc[2]) +
geom_rect(aes(xmin = date, xmax = date), ymax = params$MEAN, ymin = params$MEDIAN, color = cc[3]) +
geom_rect(aes(xmin = date, xmax = date), ymin = params$Q1, ymax = params$MEDIAN, color = cc[4]) +
geom_rect(aes(xmin = date, xmax = date, ymin = -1200), ymax = params$Q1, color = cc[5]) +
geom_area(fill = "black") +
xlim(c((Sys.Date()-365*window), Sys.Date())) + geom_line(aes(y = EMA6), color = "red", lwd = 1) +
ylim(-1200, 1200)
heat <- heat %>% mutate(cumsum = cumsum(diff))
ggplot(heat, aes(x = date, y = cumsum)) + geom_line(color = "blue") + xlim(c((Sys.Date()-365*window), Sys.Date())) + ylim(c(2.6e+05, 3.4e+05))
lookback <- 12
heat <- heat %>% mutate(RSI_cumsum = RSI(cumsum, n = 12))
heat$avg_RSI <- rollapply(heat$RSI_cumsum, width = lookback, fill = NA, align = "right", FUN = mean, na.rm = TRUE)
heat$sd_RSI <- rollapply(heat$RSI_cumsum, width = lookback, fill = NA, align = "right", FUN = sd, na.rm = TRUE)
heat <- heat %>% mutate(RSI_z = (RSI_cumsum - avg_RSI)/sd_RSI)
heat$max_z <- rollapply(heat$RSI_z, width = lookback, fill = NA, align = "right", FUN = max, na.rm = TRUE)
heat$min_z <- rollapply(heat$RSI_z, width = lookback, fill = NA, align = "right", FUN = min, na.rm = TRUE)
params <- heat %>% summarize(Q1 = quantile(RSI_cumsum, 0.25, na.rm = TRUE), MEAN = mean(RSI_cumsum, na.rm = TRUE), MEDIAN = median(RSI_cumsum, na.rm = TRUE),Q3 = quantile(RSI_cumsum, 0.75, na.rm = TRUE), IQR = IQR(RSI_cumsum, na.rm = TRUE), STDEV = sd(RSI_cumsum, na.rm = TRUE)) %>%
mutate(SKEW = ifelse(MEAN > MEDIAN, "RIGHT", "LEFT"))
RSI <- ggplot(heat, aes(x = date, y = RSI_cumsum)) +
geom_rect(aes(xmin = date, xmax = date), ymin = params$Q3, ymax = 100, color = cc[1]) +
geom_rect(aes(xmin = date, xmax = date), ymax = params$Q3, ymin = params$MEDIAN, color = cc[2]) +
geom_rect(aes(xmin = date, xmax = date), ymax = params$MEDIAN, ymin = params$MEAN, color = cc[3]) +
geom_rect(aes(xmin = date, xmax = date), ymin = params$Q1, ymax = params$MEAN, color = cc[4]) +
geom_rect(aes(xmin = date, xmax = date), ymin = 0, ymax = params$Q1, color = cc[5]) +
geom_line(color = "black") +
xlim(c((Sys.Date()-365*2), Sys.Date())) +
ylim(c(0,101))
heat <-  mutate(
heat,
index = ifelse(RSI_cumsum >= params$Q3,
((20/(100-params$Q3))*(RSI_cumsum - params$Q3)+80),
ifelse(RSI_cumsum < params$Q3 & RSI_cumsum >= params$MEDIAN,
((20/(params$Q3-params$MEDIAN))*(RSI_cumsum - params$MEDIAN)+60),
ifelse(RSI_cumsum < params$MEDIAN & RSI_cumsum >= params$MEAN,
((20/(params$MEDIAN-params$MEAN))*(RSI_cumsum - params$MEAN)+40),
ifelse(RSI_cumsum < params$MEAN & RSI_cumsum >= params$Q1,
((20/(params$MEAN-params$Q1))*(RSI_cumsum - params$Q1)+20),
((20/(params$Q1-0))*(RSI_cumsum - 0)+0))))))
ggplot(heat, aes(x = date, y = index)) +
geom_rect(aes(xmin = date, xmax = date), ymin = 80, ymax = 100, color = cc[1]) +
geom_rect(aes(xmin = date, xmax = date), ymin = 60, ymax = 80, color = cc[2]) +
geom_rect(aes(xmin = date, xmax = date), ymin = 40, ymax = 60, color = cc[3]) +
geom_rect(aes(xmin = date, xmax = date), ymin = 20, ymax = 40, color = cc[4]) +
geom_rect(aes(xmin = date, xmax = date), ymin = 0, ymax = 20, color = cc[5]) +
geom_line(color = "black") +
xlim(c((Sys.Date()-365*2), Sys.Date())) +
ylim(c(0,101))
#Today's values (RSI_cumsum is market heat)
print(glimpse(tail(heat[dim(heat)[1],])))
temp_heat <- heat
BB <- runSD(temp_heat$index)
temp_heat$BBlo <- BB
ggplot(temp_heat, aes(x = date, y = BBlo)) + geom_line() +
xlim(c((Sys.Date()-365*2), Sys.Date()))
basics <- c(as.character(0:9), as.character(letters), as.character(LETTERS), "!","@","#","$","%")
##Please enter four digit password
set.seed(3295)
code <- as.character(paste(sample(0:9, 26, replace = TRUE), collapse = ""))
GeneratePassword <- function(website, keyword, seed) {
set.seed(seed)
part1 <- paste(sample(basics,3), collapse = "")
set.seed(seed + sample(100:1000, 1))
part2 <- chartr("abcdefghijklmnopqrstuvwxyz",code, keyword)
set.seed(seed + sample(100:1000, 1))
part3 <- paste(sample(basics,3), collapse = "")
set.seed(seed + sample(100:1000, 1))
part4 <- paste(sample(basics,3), collapse = "")
paste0(part1,part2,part3,part4)
}
GeneratePassword("Reddit", "author", 123)
rm(list = ls())
cat("\014")
CIK = sub("^0*","",CIK)
library(XML)
library(RCurl)
getCIK = function(ticker) {
stopifnot(is.character(ticker))
uri = "http://www.sec.gov/cgi-bin/browse-edgar"
response = getForm(uri,CIK=ticker,action="getcompany")
html = htmlParse(response)
CIKNode = getNodeSet(html, "//acronym[@title=\"Central Index Key\"][text() = \"CIK\"]")
CIKNodeText = sapply(CIKNode, function(x) xmlValue(getSibling(getSibling(x))))
CIK = sub(" .*","",CIKNodeText)
CIK = sub("^0*","",CIK)
CIK
}
temp = getCIK("AAPL")
temp
ticker = "AAPL"
stopifnot(is.character(ticker))
uri = "http://www.sec.gov/cgi-bin/browse-edgar"
response = getForm(uri,CIK=ticker,action="getcompany")
response
response = getForm(uri,CIK=ticker,action="getcompany", owner="exclude", Find="Search")
response
html = htmlParse(response)
html
?getForm
ticker = "AAPL"
stopifnot(is.character(ticker))
directory = "http://www.sec.gov/cgi-bin/browse-edgar?"
CIK = ticker
owner = "exclude"
action = "getcompany"
Find = "Search"
final = paste0(directory, "CIK=",CIK, "&owner=",owner, "&action=",action, "&Find=",Find)
final
resp <- GET(final)
library(httr)
resp <- GET(final)
resp
install.packages("rvest")
library(rvest)
resp = html(final)
resp = read_html(final)
resp
table = resp %>%
html_nodes(".tableFile2")
table
table = resp %>%
html_nodes(".tableFile2") %>%
html_table()
table
table = table[1]
table
table = table[[1]]
table
ticker = "AAPL"
stopifnot(is.character(ticker))
directory = "http://www.sec.gov/cgi-bin/browse-edgar?"
CIK = ticker
owner = "exclude"
action = "getcompany"
Find = "Search"
type = "10"
count = "40" # Options: 100, 80, 40, 20 - 100 filings is 23 years, 40 filings is 10 years
final = paste0(directory,"action=",action,"CIK=",CIK, "&type=",10,"&owner=",owner, "count=",count)
resp = read_html(final)
table = resp %>%
html_nodes(".tableFile2") %>%
html_table()
table = table[[1]]
final
library(rvest)
ticker = "AAPL"
stopifnot(is.character(ticker))
directory = "http://www.sec.gov/cgi-bin/browse-edgar?"
CIK = ticker
owner = "exclude"
action = "getcompany"
Find = "Search"
type = "10"
count = "40" # Options: 100, 80, 40, 20 - 100 filings is 23 years, 40 filings is 10 years
final = paste0(directory,"action=",action,"&CIK=",CIK, "&type=",10,"&owner=",owner, "&count=",count)
resp = read_html(final)
table = resp %>%
html_nodes(".tableFile2") %>%
html_table()
table = table[[1]]
table
table = resp %>%
html_nodes(".tableFile2") %>%
html_table()
table
table = resp %>%
html_nodes(".tableFile2")
table
require(XML)
data <- xmlParse("https://www.sec.gov/Archives/edgar/data/91142/000119312517162429/aos-20170331.xml")
xml_data <- xmlToList(data)
resp = read_html("https://www.sec.gov/Archives/edgar/data/91142/000119312517162429/R2.htm")
table = resp %>%
html_nodes(".report") %>%
html_table()
table = table[[1]]
table
table = resp %>%
html_nodes(".report")
table
?html_table
table = resp %>%
html_nodes(".report") %>%
html_table(fill=TRUE)
table = table[[1]]
table
resp = read_html("https://www.sec.gov/Archives/edgar/data/91142/000119312517047795/R4.htm")
table = resp %>%
html_nodes(".report") %>%
html_table(fill=TRUE)
table = table[[1]]
table
resp = read_html("https://www.sec.gov/Archives/edgar/data/91142/000119312517047795/FilingSummary.xml")
resp
table = resp %>%
html_nodes(".text")
table
table = resp %>%
html_nodes("span .text")
table
library(XML)
temp = parseXML("https://www.sec.gov/Archives/edgar/data/91142/000119312517047795/FilingSummary.xml")
temp = parseXMLandAdd("https://www.sec.gov/Archives/edgar/data/91142/000119312517047795/FilingSummary.xml")
library(XML)
temp = parseXMLandAdd("https://www.sec.gov/Archives/edgar/data/91142/000119312517047795/FilingSummary.xml")
temp = parseXMLAndAdd("https://www.sec.gov/Archives/edgar/data/91142/000119312517047795/FilingSummary.xml")
temp
?read.xlsx
?read.xlsx
??read.xlsx
library(xlsx)
temp = read.xlsx("https://www.sec.gov/Archives/edgar/data/91142/000119312517047795/Financial_Report.xlsx", sheetName = "CONSOLIDATED STATEMENT OF EARNI")
library(xlsx)
download.file("https://www.sec.gov/Archives/edgar/data/91142/000119312517047795/Financial_Report.xlsx")
temp = read.xlsx("Financial_Report.xlsx", sheetName = "CONSOLIDATED STATEMENT OF EARNI")
download.file("https://www.sec.gov/Archives/edgar/data/91142/000119312517047795/Financial_Report.xlsx", destfile = "finrepo.xlsx")
temp = read.xlsx("finrepo.xlsx", sheetName = "CONSOLIDATED STATEMENT OF EARNI")
temp
download.file("https://www.sec.gov/Archives/edgar/data/91142/000119312517162429/Financial_Report.xlsx", destfile = "finrepo.xlsx")
temp = read.xlsx("finrepo.xlsx", sheetName = "CONSOLIDATED STATEMENT OF EARNI")
temp
download.file("https://www.sec.gov/Archives/edgar/data/91142/000119312517162429/Financial_Report.xlsx", destfile = "finrepo.xlsx")
temp = read.xlsx("finrepo.xlsx", sheetName = "CONSOLIDATED STATEMENT OF EARNI")
temp
download.file("https://www.sec.gov/Archives/edgar/data/91142/000119312517047795/Financial_Report.xlsx", destfile = "finrepo.xlsx")
temp = read.xlsx("finrepo.xlsx", sheetName = "CONSOLIDATED STATEMENT OF EARNI")
temp
download.file("https://www.sec.gov/Archives/edgar/data/91142/000119312517162429/Financial_Report.xlsx", destfile = "finrepo.xlsx")
getwd()
temp = read.xlsx("finrepo.xlsx", sheetName = "CONDENSED CONSOLIDATED STATEMEN")
temp
library(xlsx)
download.file("https://www.sec.gov/Archives/edgar/data/91142/000119312517162429/Financial_Report.xlsx", destfile = "/Users/cojamalo/Documents/R/Stocks/temp/finrepo.xlsx")
temp = read.xlsx("/Users/cojamalo/Documents/R/Stocks/temp/finrepo.xlsx", sheetName = "CONDENSED CONSOLIDATED STATEMEN")
temp
resp = read_html(final)
table = resp %>%
html_nodes(".tableFile2") %>%
html_table()
table = table[[1]]
table
table$Description
grep('(?:Acc-no: ).*(?:\()', "Quarterly report [Sections 13 or 15(d)]Acc-no: 0001193125-08-156421 (34 Act)  Size: 615 KB")
grep('(?:Acc-no: ).*', "Quarterly report [Sections 13 or 15(d)]Acc-no: 0001193125-08-156421 (34 Act)  Size: 615 KB")
grepl('(?:Acc-no: ).*', "Quarterly report [Sections 13 or 15(d)]Acc-no: 0001193125-08-156421 (34 Act)  Size: 615 KB")
grep('(?:Acc-no: ).*', "Quarterly report [Sections 13 or 15(d)]Acc-no: 0001193125-08-156421 (34 Act)  Size: 615 KB")
?grep
grep('(?:Acc-no: ).*', "Quarterly report [Sections 13 or 15(d)]Acc-no: 0001193125-08-156421 (34 Act)  Size: 615 KB", value=TRUE)
grep('/(?:Acc-no: ).*/g', "Quarterly report [Sections 13 or 15(d)]Acc-no: 0001193125-08-156421 (34 Act)  Size: 615 KB", value=TRUE)
grep('(?:Acc-no: )[^\s]+', "Quarterly report [Sections 13 or 15(d)]Acc-no: 0001193125-08-156421 (34 Act)  Size: 615 KB", value=TRUE)
?regex
grep('(?:Acc-no: )[^\\s]+', "Quarterly report [Sections 13 or 15(d)]Acc-no: 0001193125-08-156421 (34 Act)  Size: 615 KB", value=TRUE)
gsub('(?:Acc-no: )[^\\s]+', "Quarterly report [Sections 13 or 15(d)]Acc-no: 0001193125-08-156421 (34 Act)  Size: 615 KB", value=TRUE)
gsub('(?:Acc-no: )[^\\s]+', "Quarterly report [Sections 13 or 15(d)]Acc-no: 0001193125-08-156421 (34 Act)  Size: 615 KB")
?gsub
?sub
library(stringr)
str_extract("Quarterly report [Sections 13 or 15(d)]Acc-no: 0001193125-08-156421 (34 Act)  Size: 615 KB", '(?:Acc-no: )[^\\s]+')
str_extract("Quarterly report [Sections 13 or 15(d)]Acc-no: 0001193125-08-156421 (34 Act)  Size: 615 KB", '(?<=Acc-no: )[^\\s]+')
resp = read_html(final)
table = resp %>%
html_nodes(".tableFile2") %>%
html_table()
table = table[[1]] %>% mutate(Acc_No = str_extract(Description, '(?<=Acc-no: )[^\\s]+'))
table
table = resp %>%
html_nodes(".tableFile2") %>%
html_table()
table = table[[1]] %>% mutate(Acc_No = str_extract(Description, '(?<=Acc-no: )[^\\s]+'), Acc_No = gsub("-","",Acc_No))
table
final
setwd("/Users/cojamalo/Documents/GitHub/R-EDGAR-Data-Scraping/")
library(xlsx)
download.file("https://www.sec.gov/Archives/edgar/data/91142/000119312517162429/Financial_Report.xlsx", destfile = "finrepo.xlsx")
temp = read.xlsx("finrepo.xlsx", sheetName = "CONDENSED CONSOLIDATED STATEMEN")
temp
resp = read_html(final)
table = resp %>%
html_nodes(".tableFile2") %>%
html_table()
table = table[[1]] %>% mutate(Acc_No = str_extract(Description, '(?<=Acc-no: )[^\\s]+'), Acc_No = gsub("-","",Acc_No))
table
resp
resp %>% html_nodes(".companyName")
resp %>% html_nodes("td , .companyName , .identInfo , .companyName a")
resp %>% html_nodes("td .companyName .identInfo .companyName a")
resp %>% html_nodes("td , .companyName , .identInfo , .companyName a")
resp %>% html_nodes("#documentsbutton")
CIkcode = resp %>%
html_nodes("#documentsbutton") %>%
.[1]
CIkcode = resp %>%
html_nodes("#documentsbutton") %>%
.[[1]]
CIkcode = resp %>%
html_nodes("#documentsbutton") %>%
.[[1]] %>%
html_text()
CIkcode = resp %>%
html_nodes("#documentsbutton") %>%
.[[1]]
CIkcode
CIkcode[1]
CIkcode[[1]]
CIkcode
CIkcode = resp %>%
html_nodes("#documentsbutton") %>%
.[[1]] %>% htmlParse()
CIkcode
CIKcode = resp %>%
html_nodes("#documentsbutton") %>%
.[[1]] %>% htmltable()
CIKcode = resp %>%
html_nodes("#documentsbutton") %>%
.[[1]] %>% html_table()
CIK_code = resp %>%
html_nodes("#documentsbutton") %>%
.[[1]] %>% html_name()
CIK_code = resp %>%
html_nodes("#documentsbutton") %>%
.[[1]] %>% html()
CIK_code = resp %>%
html_nodes("#documentsbutton") %>%
.[[1]] %>% read_html()
CIK_code = resp %>%
html_nodes("#documentsbutton") %>%
.[[1]] %>% html_attr("href")
CIK_code
CIK_code = str_extract(CIK_code, '(?<=data\/)[^\/]+')
CIK_code = str_extract(CIK_code, '(?<=data\\/)[^\\/]+')
table$Acc_No[1]
base = "https://www.sec.gov/Archives/edgar/data/"
urls = c()
for (i in nrow(table)) {
accno = table$Acc_No[i]
new_url = paste0(base,CIK_code,"/",accno,"/Financial_Report.xlsx")
urls = c(urls, new_url)
}
urls
base = "https://www.sec.gov/Archives/edgar/data/"
urls = c()
for (i in nrow(table)) {
accno = table$Acc_No[i]
new_url = paste0(base,CIK_code,"/",accno,"/Financial_Report.xlsx")
urls = c(new_url, urls)
}
urls
nrow(table)
base = "https://www.sec.gov/Archives/edgar/data/"
urls = c()
for (i in nrow(table)) {
print(i)
accno = table$Acc_No[i]
new_url = paste0(base,CIK_code,"/",accno,"/Financial_Report.xlsx")
urls = c(urls, new_urls)
}
base = "https://www.sec.gov/Archives/edgar/data/"
urls = c()
for (i in 1:nrow(table)) {
accno = table$Acc_No[i]
new_url = paste0(base,CIK_code,"/",accno,"/Financial_Report.xlsx")
urls = c(urls, new_urls)
}
base = "https://www.sec.gov/Archives/edgar/data/"
urls = c()
for (i in 1:nrow(table)) {
accno = table$Acc_No[i]
new_url = paste0(base,CIK_code,"/",accno,"/Financial_Report.xlsx")
urls = c(urls, new_url)
}
urls
table
grepl("10-Q", table$Filings[1])
base = "https://www.sec.gov/Archives/edgar/data/"
Q_urls = c()
K_urls
for (i in 1:nrow(table)) {
accno = table$Acc_No[i]
new_url = paste0(base,CIK_code,"/",accno,"/Financial_Report.xlsx")
if grepl("10-Q", table$Filings[i]) {
Q_urls = c(Q_urls, new_url)
}
else if grepl("10-K", table$Filings[i]) {
K_urls = c(K_urls, new_url)
}
}
base = "https://www.sec.gov/Archives/edgar/data/"
Q_urls = c()
K_urls
for (i in 1:nrow(table)) {
accno = table$Acc_No[i]
new_url = paste0(base,CIK_code,"/",accno,"/Financial_Report.xlsx")
if (grepl("10-Q", table$Filings[i])) {
Q_urls = c(Q_urls, new_url)
}
else if (grepl("10-K", table$Filings[i])) {
K_urls = c(K_urls, new_url)
}
}
Q_urls = K_urls = c()
for (i in 1:nrow(table)) {
accno = table$Acc_No[i]
new_url = paste0(base,CIK_code,"/",accno,"/Financial_Report.xlsx")
if (grepl("10-Q", table$Filings[i])) {
Q_urls = c(Q_urls, new_url)
}
else if (grepl("10-K", table$Filings[i])) {
K_urls = c(K_urls, new_url)
}
}
Q_urls
K_urls
get_xlsx_tables(K_urls, K_sheetName)
get_xlsx_tables = function(url_list, sheetName) {
for (url in url_list) {
download.file(url, destfile = "/temp/finrepo.xlsx")
table = read.xlsx("/temp/finrepo.xlsx", sheetName = "CONSOLIDATED STATEMENT OF EARNI")
if (is.null(output)) {
output = table
}
else {
output = cbind(output, table[,-1])
}
}
return (output)
}
get_xlsx_tables(K_urls, K_sheetName)
K_ruls
K_urls
?download.file
file.exists('https://www.sec.gov/Archives/edgar/data/320193/000162828016020309/Financial_Report.xlsx')
get_xlsx_tables = function(url_list, sheetName) {
for (url in url_list) {
download.file(url, destfile = "/temp/finrepo.xlsx")
table = read.xlsx("/temp/finrepo.xlsx", sheetName = sheetName)
if (is.null(output)) {
output = table
}
else {
output = cbind(output, table[,-1])
}
}
return (output)
}
get_xlsx_tables(K_urls, K_sheetName)
for (url in K_url) {print (file.exists(url()}
for (url in K_url) {print (file.exists(url)}
for (url in K_url) { print (file.exists(url)) }
for (url in K_urls) { print (file.exists(url)) }
K_urls[1]
download.file("https://www.sec.gov/Archives/edgar/data/320193/000162828016020309/Financial_Report.xlsx", destfile = "/temp/finrepo.xlsx")
?download.file
download.file(url="https://www.sec.gov/Archives/edgar/data/320193/000162828016020309/Financial_Report.xlsx", destfile = "/temp/finrepo.xlsx")
download.file(url="https://www.sec.gov/Archives/edgar/data/320193/000162828017004790/Financial_Report.xlsx", destfile = "/temp/finrepo.xlsx")
